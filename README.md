# SearchEngine

# Introduction
  - Search Engine project will able to provide users required information they are looking for online using keywords.
  - Introducing the three stages of Search Engine project

        1: Web Crawling
        2: Indexing 
        3: Serving search results
   
 # Web Crawling
 - Web crawling is the process of systematically scanning and collecting data from websites on the internet. The process involves using web crawlers, to navigate through websites, following links and collecting data along the way.
 - Web crawlers are automated programs that visit websites and extract information such as text, images, and links. They start by visiting a seed URL, which is typically a homepage or a list of URLs, and then follow links to other pages on the website. As they crawl through the website, they collect data from each page and store it in a databas.
 - 
 
